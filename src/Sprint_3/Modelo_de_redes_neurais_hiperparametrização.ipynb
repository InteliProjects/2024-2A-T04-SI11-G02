{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo para detecção de fraudes"
      ],
      "metadata": {
        "id": "hOtzzD2g3ixI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O objetivo deste projeto é desenvolver um modelo de rede neural capaz de prever fraudes no consumo de água. Essa tarefa é de extrema importância, pois fraudes podem representar perdas significativas para empresas de saneamento. Para isso, utilizamos uma combinação de técnicas avançadas de aprendizado de máquina e estratégias de otimização de hiperparâmetros. O relatório detalha as etapas tomadas, com foco nas escolhas de hiperparâmetros e o impacto que essas escolhas tiveram no desempenho final do modelo.\n",
        "\n",
        "Este relatório está dividido em três partes principais:\n",
        "- **Otimização de Hiperparâmetros**: Análise das estratégias utilizadas e impacto no desempenho.\n",
        "- **Análise de Impacto**: Discussão detalhada sobre como cada hiperparâmetro influencia a performance.\n",
        "- **Resultados Finais e Conclusões**: Apresentação do modelo otimizado e justificativa das escolhas.\n"
      ],
      "metadata": {
        "id": "b9uxC0Tl3d-m"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0J1Q5fQvpkr"
      },
      "source": [
        "## Instalação e download de dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ED_dnBRN-6g0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "import tensorflow_addons as tfa\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Importa o regressor de processo gaussiano, que modela a função de perda como uma distribuição probabilística\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "\n",
        "# Importa o kernel Matern, que define como dois pontos são considerados similares em um processo gaussiano\n",
        "from sklearn.gaussian_process.kernels import Matern\n",
        "\n",
        "# Função principal da otimização bayesiana que minimiza a função objetivo de forma eficiente\n",
        "from skopt import gp_minimize # type: ignore\n",
        "\n",
        "# Importa definições para criar o espaço de busca de hiperparâmetros (números reais e inteiros)\n",
        "from skopt.space import Real, Integer  # type: ignore\n",
        "\n",
        "\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94UZao_vSnbW"
      },
      "outputs": [],
      "source": [
        "!gdown 1iZDFEefN0J2wao1QRIAuT-ukp0coX0Rp\n",
        "!gdown 1_lY6ydxyDA9-HNrYleq4UrL-JKcKLM7c\n",
        "!gdown 1C1ZHPeYF71NVVOkZD6cW5SmzV9Uui4QK\n",
        "!gdown 1Lbayox3-fo92nLSvk1MPbai5ek0Noqi3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRRcqJWA0KIC"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/DADOS_PROCESSADOS (1).csv', delimiter=',')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQfTqsWH-06J"
      },
      "outputs": [],
      "source": [
        "df_ext = pd.read_csv('/content/DADOS_PROCESSADOS_COMPLETOS.csv', delimiter=',')\n",
        "df_ext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGYc7HsTv50F"
      },
      "source": [
        "Os dados utilizados no modelo apresentado a seguir foram selecionados com base nos insights obtidos durante a análise exploratória. Decidimos focar em três variáveis principais: consumo, localização e categoria. Essa escolha foi motivada pelo nosso objetivo de avaliar se essas variáveis isoladas seriam suficientes para que o modelo compreendesse os padrões de consumo e comportamento dos clientes da Aegea.\n",
        "\n",
        "Os resultados preliminares indicaram que o modelo foi capaz de alcançar uma performance razoável utilizando apenas essas três variáveis. No entanto, durante a apresentação, nosso parceiro de projeto expressou preocupação com a limitação imposta pela utilização de apenas três variáveis. Reconhecemos a validade dessa observação e, embora tenhamos decidido manter a abordagem inicial para fins comparativos, planejamos incorporar outras variáveis em etapas subsequentes para aprimorar o desenvolvimento do modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qk8SkTlFHQbk"
      },
      "outputs": [],
      "source": [
        "df = df.drop(columns=['MES_13', 'MES_14', 'MES_15',\n",
        "       'MES_16', 'MES_17', 'MES_18', 'MES_19', 'MES_20', 'MES_21', 'MES_22',\n",
        "       'MES_23', 'MES_24', 'MES_25', 'MES_26', 'MES_27', 'MES_28', 'MES_29',\n",
        "       'MES_30', 'MES_31', 'MES_32', 'MES_33', 'MES_34', 'MES_35', 'MES_36'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yYe11a7_GuZ"
      },
      "outputs": [],
      "source": [
        "df_ext = df_ext.drop(columns=['MATRICULA', 'CONS_MEDIDO', 'ANOMES', 'COD_LATITUDE', 'COD_LONGITUDE', 'CLUSTER', 'CONTAGEM_MATRICULA','MES_7', 'MES_8', 'MES_9', 'MES_10', 'MES_11', 'MES_12', 'MES_13', 'MES_14', 'MES_15', 'ECO_INDUSTRIAL', 'ECO_COMERCIAL', 'ECO_PUBLICA', 'ECO_OUTRAS',    'COD_GRUPO', 'COD_SETOR_COMERCIAL', 'COD_SETOR_COMERCIAL.1',\n",
        "       'MES_16', 'MES_17', 'MES_18', 'MES_19', 'MES_20', 'MES_21', 'MES_22',\n",
        "       'MES_23', 'MES_24', 'MES_25', 'MES_26', 'MES_27', 'MES_28', 'MES_29',\n",
        "       'MES_30', 'MES_31', 'MES_32', 'MES_33', 'MES_34', 'MES_35', 'MES_36'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DUBcXU3zi6G"
      },
      "source": [
        "Aqui, dividimos a coluna target das outras colunas utilizadas como features do moddelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMDYsvUDAVdB"
      },
      "outputs": [],
      "source": [
        "X = df.drop(columns=['FRAUDADOR'])\n",
        "y = df['FRAUDADOR']\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Me3RHJbJAGKX"
      },
      "outputs": [],
      "source": [
        "X_ext = df_ext.drop(columns=['FRAUDADOR'])\n",
        "y_ext = df_ext['FRAUDADOR']\n",
        "X_ext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqQRPg3d-sCP"
      },
      "source": [
        "## Equilíbrio de classes por meio do método undersampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bsy97i_BNNRi"
      },
      "outputs": [],
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Analisando o desequilíbrio das classes\n",
        "y.value_counts().plot(kind='bar')\n",
        "plt.title('Distribuição das Classes')\n",
        "plt.show()\n",
        "\n",
        "# Aplicando RandomUnderSampler para balanceamento\n",
        "undersampler = RandomUnderSampler(random_state=42)\n",
        "X_balanced, y_balanced = undersampler.fit_resample(X, y)\n",
        "\n",
        "# Verificando a nova distribuição das classes\n",
        "y_balanced.value_counts().plot(kind='bar')\n",
        "plt.title('Distribuição das Classes Após Undersampling')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yt9cGXzfAOXs"
      },
      "outputs": [],
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Analisando o desequilíbrio das classes\n",
        "y_ext.value_counts().plot(kind='bar')\n",
        "plt.title('Distribuição das Classes')\n",
        "plt.show()\n",
        "\n",
        "# Aplicando RandomUnderSampler para balanceamento\n",
        "undersampler = RandomUnderSampler(random_state=42)\n",
        "X_balanced_ext, y_balanced_ext = undersampler.fit_resample(X_ext, y_ext)\n",
        "\n",
        "# Verificando a nova distribuição das classes\n",
        "y_balanced_ext.value_counts().plot(kind='bar')\n",
        "plt.title('Distribuição das Classes Após Undersampling')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvNamHDzVZXQ"
      },
      "source": [
        "## Arquitetura e estruturação do modelo de redes neurais"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O modelo de rede neural escolhido segue uma arquitetura piramidal, na qual o número de neurônios em cada camada subsequente diminui, criando um funil de informações que permite ao modelo captar padrões mais complexos nas camadas superiores e refinar os detalhes nas camadas mais profundas.\n",
        "\n",
        "A função de ativação **ReLU (Rectified Linear Unit)** foi selecionada para as camadas ocultas devido à sua eficiência em evitar o problema de vanishing gradient, permitindo que o modelo aprenda de forma mais eficiente em problemas de classificação. A última camada usa a função **sigmoid**, que transforma a saída do modelo em uma probabilidade, apropriada para problemas binários como este.\n"
      ],
      "metadata": {
        "id": "S4FCXGI63q_t"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAN8turL1y6g"
      },
      "source": [
        "Divisão dos dados em treino, teste e validação."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5QZhlSJDfln"
      },
      "outputs": [],
      "source": [
        "# Primeiro, divisão dos dados em treino (70%) e teste (30%)\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42)\n",
        "\n",
        "# Agora, dividimos o conjunto de treino em treino (70% do total) e validação (30% do total)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiddpwCTAZvf"
      },
      "outputs": [],
      "source": [
        "# Primeiro, divisão dos dados em treino (70%) e teste (30%)\n",
        "X_train_full_ext, X_test_ext, y_train_full_ext, y_test_ext = train_test_split(X_balanced_ext, y_balanced_ext, test_size=0.3, random_state=42)\n",
        "\n",
        "# Agora, dividimos o conjunto de treino em treino (70% do total) e validação (30% do total)\n",
        "X_train_ext, X_val_ext, y_train_ext, y_val_ext = train_test_split(X_train_full_ext, y_train_full_ext, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeZe6QhM2eZR"
      },
      "source": [
        "Desenvolvemos um modelo de rede neural utilizando uma arquitetura piramidal, onde o número de neurônios em cada camada corresponde a potências de dois. Essa escolha visa explorar a eficiência e a capacidade de generalização dessa estrutura.\n",
        "\n",
        "A camada de saída do modelo foi configurada para gerar valores probabilísticos entre 0 e 1, o que nos levou a optar pela função de perda 'binary crossentropy'. Esta função é ideal para medir a discrepância entre as probabilidades previstas pelo modelo e as classes reais binárias, auxiliando no ajuste eficaz dos pesos durante o treinamento."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criação do modelo"
      ],
      "metadata": {
        "id": "kD1-tRZ89FAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# Compilando o modelo\n",
        "model.compile(loss='binary_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "\n",
        "# Resumo da arquitetura do modelov\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "rtLfEZL59Gdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ext = Sequential()\n",
        "model_ext.add(Dense(128, input_dim=X_train_ext.shape[1], activation='relu'))\n",
        "model_ext.add(Dense(64, activation='relu'))\n",
        "model_ext.add(Dense(32, activation='relu'))\n",
        "model_ext.add(Dense(16, activation='relu'))\n",
        "model_ext.add(Dense(8, activation='relu'))\n",
        "model_ext.add(Dense(1, activation='sigmoid'))\n",
        "# Compilando o modelo\n",
        "model_ext.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "\n",
        "# Resumo da arquitetura do modelov\n",
        "model_ext.summary()"
      ],
      "metadata": {
        "id": "s_oKxIoy9Ku6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWE2W92zVfEK"
      },
      "source": [
        "## Compilação do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1W-wVWIFG4L8"
      },
      "outputs": [],
      "source": [
        "optimizer = RMSprop(learning_rate=0.001)\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='RMSprop',\n",
        "              metrics=[\n",
        "                  'accuracy',\n",
        "                  tfa.metrics.F1Score(num_classes=1, threshold=0.5),\n",
        "                  tf.keras.metrics.Precision(),\n",
        "                  tf.keras.metrics.Recall()\n",
        "              ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRJfMyjXCFEl"
      },
      "outputs": [],
      "source": [
        "optimizer = RMSprop(learning_rate=0.0001)\n",
        "\n",
        "model_ext.compile(loss='binary_crossentropy',\n",
        "              optimizer='RMSprop',\n",
        "              metrics=[\n",
        "                  'accuracy',\n",
        "                  tfa.metrics.F1Score(num_classes=1, threshold=0.5),\n",
        "                  tf.keras.metrics.Precision(),\n",
        "                  tf.keras.metrics.Recall()\n",
        "              ])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinamento do modelo"
      ],
      "metadata": {
        "id": "pgd3F_DlbUEi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIoPCs9nG7B5"
      },
      "outputs": [],
      "source": [
        "# Definir EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "# class_weight = {0: 1.5, 1: 2}  # Pese mais a classe positiva (classe 1)\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    callbacks=[early_stopping],\n",
        "                    # class_weight=class_weight\n",
        "                    )\n",
        "\n",
        "# # Avaliar o modelo no conjunto de teste\n",
        "loss, accuracy, f1_score, precision, recall = model.evaluate(X_test, y_test)\n",
        "print(f'Loss: {loss}, Accuracy: {accuracy}, F1-Score: {f1_score}, Precision: {precision}, Recall: {recall}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6-H4pvPAxRC"
      },
      "outputs": [],
      "source": [
        "# Definir EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "# class_weight = {0: 1, 1: 1.3}  # Pese mais a classe positiva (classe 1)\n",
        "history_ext = model_ext.fit(X_train_ext, y_train_ext, epochs=100, batch_size=32,\n",
        "                    validation_data=(X_test_ext, y_test_ext),\n",
        "                    callbacks=[early_stopping],\n",
        "                    # class_weight=class_weight\n",
        "                    )\n",
        "\n",
        "# # Avaliar o modelo no conjunto de teste\n",
        "loss, accuracy, f1_score, precision, recall = model_ext.evaluate(X_test_ext, y_test_ext)\n",
        "print(f'Loss: {loss}, Accuracy: {accuracy}, F1-Score: {f1_score}, Precision: {precision}, Recall: {recall}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvTVWyptVkf9"
      },
      "source": [
        "## Avaliação da performance do modelo no conjunto de validação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qPDs0Rk78gP"
      },
      "outputs": [],
      "source": [
        "# Avaliar o modelo no conjunto de teste\n",
        "results = model.evaluate(X_test, y_test)\n",
        "test_loss, test_accuracy, test_f1_score, test_precision, test_recall = results\n",
        "print(f'Conjunto de Teste - Loss: {test_loss}, Accuracy: {test_accuracy}, F1-Score: {test_f1_score}, Precision: {test_precision}, Recall: {test_recall}')\n",
        "\n",
        "# Avaliar o modelo no conjunto de validação\n",
        "results_val = model.evaluate(X_val, y_val)\n",
        "val_loss, val_accuracy, val_f1_score, val_precision, val_recall = results_val\n",
        "print(f'Conjunto de Validação - Loss: {val_loss}, Accuracy: {val_accuracy}, F1-Score: {val_f1_score}, Precision: {val_precision}, Recall: {val_recall}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gauH0iHBB6H"
      },
      "outputs": [],
      "source": [
        "# Avaliar o modelo no conjunto de teste\n",
        "results = model_ext.evaluate(X_test_ext, y_test_ext)\n",
        "test_loss, test_accuracy, test_f1_score, test_precision, test_recall = results\n",
        "print(f'Conjunto de Teste - Loss: {test_loss}, Accuracy: {test_accuracy}, F1-Score: {test_f1_score}, Precision: {test_precision}, Recall: {test_recall}')\n",
        "\n",
        "# Avaliar o modelo no conjunto de validação\n",
        "results_val = model_ext.evaluate(X_val_ext, y_val_ext)\n",
        "val_loss, val_accuracy, val_f1_score, val_precision, val_recall = results_val\n",
        "print(f'Conjunto de Validação - Loss: {val_loss}, Accuracy: {val_accuracy}, F1-Score: {val_f1_score}, Precision: {val_precision}, Recall: {val_recall}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hiperparametrização"
      ],
      "metadata": {
        "id": "ZVca7MK7a_fQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A otimização de hiperparâmetros é uma etapa crucial no desenvolvimento de modelos de rede neural, pois os valores corretos podem aumentar significativamente o desempenho do modelo. Para esse projeto, optamos por duas abordagens complementares:\n",
        "\n",
        "1. **RandomizedSearchCV**: Explora aleatoriamente o espaço de hiperparâmetros. Essa técnica é eficiente quando o espaço de busca é grande, pois evita a exaustividade do grid search e cobre uma amostra representativa dos parâmetros.\n",
        "2. **Bayesian Optimization (com skopt)**: É uma abordagem mais sofisticada, que constrói um modelo probabilístico do espaço de busca e usa esse modelo para selecionar as amostras subsequentes. Esta técnica é especialmente útil quando o custo computacional é alto, pois reduz o número de iterações necessárias para encontrar bons resultados.\n",
        "\n",
        "Utilizamos essas duas técnicas para otimizar os seguintes hiperparâmetros:\n",
        "- **Número de neurônios por camada**: Testamos entre 16 e 256 neurônios.\n",
        "- **Taxa de aprendizado**: Variamos a taxa de aprendizado de 1e-5 a 1e-2.\n",
        "- **Funções de ativação**: Testamos ReLU, sigmoid, tanh e softmax.\n",
        "- **Taxa de dropout**: Para prevenir overfitting, testamos diferentes taxas de dropout.\n",
        "- **Otimizador**: Comparando entre Adam, RMSprop, e SGD.\n"
      ],
      "metadata": {
        "id": "11DOsls33_di"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytPbO1HKX94s"
      },
      "outputs": [],
      "source": [
        "# from skopt.space import Categorical\n",
        "\n",
        "# # Definindo o espaço de busca\n",
        "# space2 = [\n",
        "#     Real(1e-5, 1e-2, name='learning_rate'),       # Espaço para a taxa de aprendizado\n",
        "#     Integer(16, 258, name='num_neurons'),          # Espaço para o número de neurônios\n",
        "#     Integer(16, 256, name='batch_size'),          # Espaço para o tamanho do batch\n",
        "#     Integer(1, 10, name='num_layers'),             # Número de camadas\n",
        "#     Categorical(['relu', 'tanh', 'sigmoid','softmax'], name='activation'),  # Funções de ativação\n",
        "#     Real(0.0, 1, name='dropout_rate'),          # Taxa de dropout\n",
        "#     Categorical(['adam', 'sgd', 'rmsprop'], name='optimizer')     # Otimizador\n",
        "# ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNy5dqkHXMPi"
      },
      "outputs": [],
      "source": [
        "# # Função de avaliação para a Otimização Bayesiana\n",
        "# def train_eval(params):\n",
        "#     learning_rate, num_neurons, batch_size, num_layers, activation, dropout_rate, optimizer = params\n",
        "\n",
        "#     # Criar o modelo\n",
        "#     model = create_model(learning_rate, num_neurons)\n",
        "\n",
        "#     # Treinar o modelo\n",
        "#     history = model.fit(X_train, y_train, epochs=50, batch_size=10, verbose=0)\n",
        "\n",
        "#     # Avaliar o modelo\n",
        "#     _, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "#     # A função de perda para a otimização bayesiana precisa ser minimizada, então retornamos 1 - accuracy\n",
        "#     return 1 - accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zElunp-aYNiu"
      },
      "outputs": [],
      "source": [
        "# # Executando a Otimização Bayesiana\n",
        "# result2 = gp_minimize(train_eval, space2, n_calls=10, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktq4Sw3YYRR4"
      },
      "outputs": [],
      "source": [
        "# 1# Exibir os melhores hiperparâmetros encontrados\n",
        "# print(f\"Melhor taxa de aprendizado: {result2.x[0]}\")\n",
        "# print(f\"Melhor número de neurônios: {result2.x[1]}\")\n",
        "# print(f\"Melhor resultado (1 - accuracy): {result2.fun}\")\n",
        "# print(f\"Melhor batch size: {result2.x[2]}\")\n",
        "# print(f\"Melhor número de camadas: {result2.x[3]}\")\n",
        "# print(f\"Melhor função de ativação: {result2.x[4]}\")\n",
        "# print(f\"Melhor taxa de dropout: {result2.x[5]}\")\n",
        "# print(f\"Melhor otimizador: {result2.x[6]}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scikeras.wrappers import KerasClassifier"
      ],
      "metadata": {
        "id": "V_eHGPey2Hw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import uniform, randint\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "BFJACNmGZ26q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para criar o modelo Keras\n",
        "def create_model(num_neurons=32, num_layers=4, activation='relu', dropout_rate=0.0, learning_rate=0.001, optimizer='adam'):\n",
        "    model = Sequential()\n",
        "    model.add(tf.keras.Input(shape=(X_train.shape[1],)))\n",
        "\n",
        "    for _ in range(num_layers):\n",
        "        model.add(Dense(num_neurons, activation=activation))\n",
        "        if dropout_rate > 0.0:\n",
        "            model.add(Dropout(dropout_rate))\n",
        "\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Escolher o otimizador\n",
        "    if optimizer == 'adam':\n",
        "        opt = Adam(learning_rate=learning_rate)\n",
        "    elif optimizer == 'sgd':\n",
        "        opt = SGD(learning_rate=learning_rate)\n",
        "    elif optimizer == 'rmsprop':\n",
        "        opt = RMSprop(learning_rate=learning_rate)\n",
        "\n",
        "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Preparar o KerasClassifier com os parâmetros para o Random Search\n",
        "model = KerasClassifier(model=create_model, verbose=0)\n",
        "\n",
        "# Definir os hiperparâmetros a serem testados no Random Search\n",
        "param_dist = {\n",
        "    'model__learning_rate': uniform(1e-5, 1e-2),      # Amostra valores para taxa de aprendizado\n",
        "    'model__num_neurons': randint(16, 256),           # Amostra valores entre 16 e 256 para o número de neurônios\n",
        "    'model__num_layers': randint(1, 20),              # Amostra valores entre 1 e 10 para o número de camadas\n",
        "    'model__activation': ['relu', 'tanh', 'sigmoid', 'softmax'], # Escolher entre essas funções de ativação\n",
        "    'model__dropout_rate': uniform(0.0, 1),         # Amostra valores para a taxa de dropout\n",
        "    'model__optimizer': ['adam', 'sgd', 'rmsprop', 'lion'],   # Escolher entre esses otimizadores\n",
        "    'batch_size': randint(16, 256),                   # Amostra valores para o tamanho do batch entre 16 e 256\n",
        "    'epochs': [200]                                    # Definimos um número fixo de épocas como 50\n",
        "}\n",
        "\n",
        "# Executar o Randomized Search\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=50, scoring='accuracy', cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Treinamento e busca dos melhores parâmetros\n",
        "random_search_result = random_search.fit(X_train, y_train)\n",
        "\n",
        "# Exibir os melhores parâmetros encontrados\n",
        "best_params = random_search_result.best_params_\n",
        "print(f\"Melhores Hiperparâmetros: {best_params}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "VOsS_QkLuQZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Análise de impacto dos hiperparâmetros\n",
        "\n",
        "A seguir, discutimos o impacto de cada um dos principais hiperparâmetros no desempenho do modelo:\n",
        "\n"
      ],
      "metadata": {
        "id": "O3-TFWwV4ZSX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Melhores Hiperparâmetros: {'batch_size': 79, 'epochs': 50, 'model__activation': 'relu', 'model__dropout_rate': 0.023225206359998862, 'model__learning_rate': 0.006085448519014384, 'model__num_layers': 5, 'model__num_neurons': 88, 'model__optimizer': 'rmsprop'}"
      ],
      "metadata": {
        "id": "WZhxTSeqfKBd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Número de Neurônios**: Camadas com mais neurônios tendem a capturar padrões mais complexos, porém podem levar ao overfitting. Após a otimização, descobrimos que 88 neurônios por camada oferecia o melhor trade-off entre complexidade e generalização.\n",
        "  \n",
        "- **Taxa de Aprendizado**: Uma taxa de aprendizado muito alta pode levar o modelo a saltar para longe do ótimo global, enquanto uma taxa muito baixa pode fazer com que o treinamento seja muito lento. A taxa de 0.006 foi a que produziu o melhor desempenho, permitindo que o modelo convergisse rapidamente sem perder a capacidade de generalização.\n",
        "\n",
        "- **Função de Ativação**: A função **ReLU** se mostrou superior em termos de performance geral, principalmente devido à sua capacidade de lidar bem com problemas de gradiente.\n",
        "\n",
        "- **Taxa de Dropout**: Introduzimos uma taxa de dropout de 0.023 para prevenir overfitting, o que ajudou a melhorar a generalização do modelo.\n",
        "\n",
        "- **Otimizador**: O **RMSprop** se mostrou o melhor otimizador para o problema, devido à sua capacidade de ajustar dinamicamente a taxa de aprendizado durante o treinamento."
      ],
      "metadata": {
        "id": "M8Wm-hVU4g5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_params"
      ],
      "metadata": {
        "id": "y7UF6sXivTt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Salvar o modelo otimizado\n",
        "best_model = random_search_result.best_estimator_\n",
        "joblib.dump(best_model, 'best_model.pkl')\n",
        "\n",
        "# Fazer previsões no conjunto de teste\n",
        "y_pred_prob = best_model.predict(X_test)\n",
        "y_pred = (y_pred_prob > 0.5).astype(\"int32\")\n",
        "\n",
        "# Avaliar o desempenho final\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(\"Relatório de Classificação:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Matriz de Confusão\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Matriz de Confusão:\")\n",
        "print(cm)"
      ],
      "metadata": {
        "id": "ATRRqH7yZejQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Justificativa do modelo otimizado\n",
        "\n",
        "Após a otimização dos hiperparâmetros, o modelo final apresentou os seguintes hiperparâmetros ótimos:\n",
        "- **Número de neurônios**: 88\n",
        "- **Taxa de aprendizado**: 0.006\n",
        "- **Função de ativação**: ReLU\n",
        "- **Taxa de dropout**: 0.023\n",
        "- **Otimizador**: RMSprop\n",
        "\n",
        "Os resultados mostraram uma melhora significativa nas métricas de desempenho, especialmente na precisão e no F1-Score, indicando que o modelo foi capaz de lidar bem com o desequilíbrio de classes e capturar os padrões relacionados às fraudes no consumo de água."
      ],
      "metadata": {
        "id": "krqZTb8Z4kXC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZ60iWbfYNSz"
      },
      "source": [
        "## Visualização gráfica"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLTpoy9TYTBs"
      },
      "source": [
        "Gráfico de linha com as épocas ao longo do tempo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGkANtn-IXMe"
      },
      "outputs": [],
      "source": [
        "epochs = list(range(1, len(history.history['loss']) + 1))\n",
        "history_df = pd.DataFrame({\n",
        "    'Epoch': epochs,\n",
        "    'Training Loss': history.history['loss'],\n",
        "    'Validation Loss': history.history['val_loss'],\n",
        "    'Training Accuracy': history.history['accuracy'],\n",
        "    'Validation Accuracy': history.history['val_accuracy']\n",
        "})\n",
        "\n",
        "fig_loss = px.line(history_df, x='Epoch', y=['Training Loss', 'Validation Loss'],\n",
        "                   labels={'value': 'Loss', 'variable': 'Type'},\n",
        "                   title='Loss during Training')\n",
        "fig_loss.show()\n",
        "\n",
        "fig_accuracy = px.line(history_df, x='Epoch', y=['Training Accuracy', 'Validation Accuracy'],\n",
        "                       labels={'value': 'Accuracy', 'variable': 'Type'},\n",
        "                       title='Accuracy during Training')\n",
        "fig_accuracy.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JMg8zItbTyc"
      },
      "outputs": [],
      "source": [
        "epochs = list(range(1, len(history.history['loss']) + 1))\n",
        "history_df_ext = pd.DataFrame({\n",
        "    'Epoch': epochs,\n",
        "    'Training Loss': history.history['loss'],\n",
        "    'Validation Loss': history.history['val_loss'],\n",
        "    'Training Accuracy': history.history['accuracy'],\n",
        "    'Validation Accuracy': history.history['val_accuracy']\n",
        "})\n",
        "\n",
        "fig_loss = px.line(history_df_ext, x='Epoch', y=['Training Loss', 'Validation Loss'],\n",
        "                   labels={'value': 'Loss', 'variable': 'Type'},\n",
        "                   title='Loss during Training')\n",
        "fig_loss.show()\n",
        "\n",
        "fig_accuracy = px.line(history_df_ext, x='Epoch', y=['Training Accuracy', 'Validation Accuracy'],\n",
        "                       labels={'value': 'Accuracy', 'variable': 'Type'},\n",
        "                       title='Accuracy during Training')\n",
        "fig_accuracy.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M91Id57wYWnM"
      },
      "source": [
        "Matriz de confusão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9DAfMTj2H39"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Supondo que você já tenha treinado o modelo com o código anterior\n",
        "\n",
        "# Fazer previsões no conjunto de teste\n",
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred = (y_pred_prob > 0.5).astype(\"int32\")\n",
        "\n",
        "# Calcular a matriz de confusão\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Criar um DataFrame a partir da matriz de confusão para facilitar a visualização com Plotly\n",
        "cm_df = pd.DataFrame(cm, index=['Classe 0', 'Classe 1'], columns=['Predito 0', 'Predito 1'])\n",
        "\n",
        "# Plotar a matriz de confusão usando Plotly Express\n",
        "fig = px.imshow(cm_df, text_auto=True, color_continuous_scale='Blues', aspect='auto')\n",
        "fig.update_layout(\n",
        "    title='Matriz de Confusão',\n",
        "    xaxis_title='Predição',\n",
        "    yaxis_title='Verdadeiro',\n",
        "    coloraxis_showscale=False\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WybNNF20b3sa"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Fazer previsões no conjunto de teste\n",
        "y_pred_prob_ext = model_ext.predict(X_test_ext)\n",
        "y_pred_ext = (y_pred_prob_ext > 0.5).astype(\"int32\")\n",
        "\n",
        "# Calcular a matriz de confusão\n",
        "cm = confusion_matrix(y_test_ext, y_pred_ext)\n",
        "\n",
        "# Criar um DataFrame a partir da matriz de confusão para facilitar a visualização com Plotly\n",
        "cm_df = pd.DataFrame(cm, index=['Classe 0', 'Classe 1'], columns=['Predito 0', 'Predito 1'])\n",
        "\n",
        "# Plotar a matriz de confusão usando Plotly Express\n",
        "fig = px.imshow(cm_df, text_auto=True, color_continuous_scale='Blues', aspect='auto')\n",
        "fig.update_layout(\n",
        "    title='Matriz de Confusão',\n",
        "    xaxis_title='Predição',\n",
        "    yaxis_title='Verdadeiro',\n",
        "    coloraxis_showscale=False\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A otimização de hiperparâmetros é um passo fundamental no desenvolvimento de modelos de aprendizado de máquina, especialmente em redes neurais, onde o ajuste fino de variáveis específicas pode afetar drasticamente o desempenho. Em termos gerais, hiperparâmetros são variáveis que controlam o processo de aprendizado de um modelo, mas que não são ajustados automaticamente durante o treinamento, como é o caso dos parâmetros de um modelo (ex: pesos das conexões em uma rede neural). Eles precisam ser definidos manualmente ou otimizados através de técnicas específicas.\n",
        "\n",
        "Alguns dos hiperparâmetros mais importantes em redes neurais incluem:\n",
        "\n",
        "Número de Neurônios em Cada Camada: Refere-se à quantidade de unidades processadoras de informação em cada camada da rede. Mais neurônios podem permitir que o modelo capture padrões mais complexos, mas podem aumentar o risco de overfitting e torná-lo mais suscetível a memorizar o conjunto de treino sem generalizar bem para novos dados.\n",
        "\n",
        "Número de Camadas: A profundidade de uma rede neural, ou seja, quantas camadas ocultas o modelo possui, também afeta sua capacidade de aprendizado. Redes mais profundas conseguem representar padrões mais abstratos e complexos, mas também são mais difíceis de treinar e exigem mais recursos computacionais.\n",
        "\n",
        "Taxa de Aprendizado (Learning Rate): Controla o tamanho do passo que o algoritmo de otimização dá em direção ao mínimo da função de perda durante o treinamento. Uma taxa de aprendizado muito alta pode fazer com que o modelo \"pule\" o ótimo global, enquanto uma taxa muito baixa pode levar a um treinamento excessivamente longo ou à convergência para um mínimo local de baixa qualidade.\n",
        "\n",
        "Função de Ativação: Define como a informação é processada em cada neurônio. Algumas das funções mais comuns são ReLU (Rectified Linear Unit), que é amplamente utilizada devido à sua eficiência e ao fato de reduzir problemas como o vanishing gradient, além de funções como sigmoid, tanh e softmax, que têm diferentes vantagens dependendo da natureza do problema.\n",
        "\n",
        "Taxa de Dropout: Dropout é uma técnica de regularização que desativa aleatoriamente uma fração dos neurônios durante o treinamento para prevenir overfitting. A escolha da taxa de dropout ideal garante que o modelo não dependa de neurônios específicos e, ao mesmo tempo, não descarte neurônios em excesso, o que poderia prejudicar o aprendizado.\n",
        "\n",
        "Otimizador: O algoritmo utilizado para minimizar a função de perda. Entre os mais comuns estão Adam, RMSprop e SGD (Stochastic Gradient Descent). Cada um tem características diferentes em termos de como ajustam os pesos da rede durante o treinamento, e a escolha depende das características do problema e da natureza do dataset.\n",
        "\n",
        "Ao trabalhar com tantos hiperparâmetros, surge a questão de como ajustá-los da maneira mais eficiente possível para obter um desempenho otimizado. Uma abordagem simples seria tentar todas as combinações possíveis de hiperparâmetros (conhecida como grid search), mas essa abordagem se torna rapidamente impraticável quando há muitas combinações possíveis, pois exige um tempo de processamento muito longo. Por isso, adotamos o Random Search como a estratégia de hiperparametrização para este projeto.\n",
        "\n",
        "Por que Escolhemos o Random Search?\n",
        "Random Search é uma abordagem de otimização que, ao contrário do grid search, não explora sistematicamente todas as combinações de hiperparâmetros. Em vez disso, ele seleciona aleatoriamente um subconjunto de combinações, e o modelo é treinado e avaliado com base nessas amostras. Existem várias razões pelas quais o Random Search é especialmente eficiente para a otimização de redes neurais em comparação com outras técnicas:\n",
        "\n",
        "Maior Eficiência em Espaços Altamente Dimensionalizados: Redes neurais possuem um grande número de hiperparâmetros, e muitas vezes apenas algumas dessas combinações de hiperparâmetros têm um impacto significativo no desempenho do modelo. O Random Search tende a explorar mais variações desses hiperparâmetros críticos em vez de gastar tempo com combinações menos relevantes, como acontece no grid search.\n",
        "\n",
        "Cobertura Mais Eficiente do Espaço de Busca: Enquanto o grid search tende a testar todas as combinações de forma sistemática, ele pode desperdiçar muito esforço testando combinações de hiperparâmetros que são muito similares entre si, o que não agrega muito ao desempenho do modelo. O Random Search, por outro lado, distribui as tentativas de forma mais dispersa, o que muitas vezes resulta em melhores combinações com menos iterações.\n",
        "\n",
        "Escalabilidade: Em problemas com muitos hiperparâmetros, como o nosso caso de redes neurais, a quantidade de combinações possíveis pode crescer exponencialmente. O Random Search é escalável porque você pode limitar o número de amostras que deseja testar, economizando tempo computacional e recursos. É uma abordagem que permite encontrar bons resultados com um número significativamente menor de testes do que o grid search.\n",
        "\n",
        "Distribuição de Hiperparâmetros: Em muitas situações, algumas distribuições de hiperparâmetros (como a taxa de aprendizado ou a taxa de dropout) têm uma importância maior em intervalos específicos. O Random Search permite testar intervalos não uniformes com mais eficiência, garantindo que mais tentativas sejam feitas em regiões de maior importância para o modelo.\n",
        "\n",
        "Flexibilidade para Ajustes Futuros: Como o Random Search é relativamente simples de configurar e ajustar, ele oferece flexibilidade para incorporar novos hiperparâmetros ou alterar o intervalo dos parâmetros existentes com facilidade. Isso é uma grande vantagem em projetos iterativos, onde a natureza do problema pode evoluir ao longo do tempo.\n"
      ],
      "metadata": {
        "id": "QLMEMT946-KW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusão\n",
        "\n",
        "Esta etapa demonstra a importância de uma abordagem cuidadosa e iterativa para a construção e otimização de redes neurais em problemas críticos como a detecção de fraudes. Através da aplicação de técnicas de otimização avançadas como **RandomizedSearchCV** e de outras mais simples como a **Bayesian Optimization**, fomos capazes de melhorar, mesmo que pouco, o desempenho do modelo, tornando-o um pouco mais adequado para detectar fraudes no consumo de água com alta precisão.\n",
        "\n",
        "As técnicas e estratégias utilizadas neste projeto não apenas otimizam o desempenho do modelo, mas também buscam garantir que ele seja robusto e capaz de generalizar para novos dados. O equilíbrio entre complexidade do modelo e prevenção de overfitting foi um dos pontos centrais desta abordagem.\n"
      ],
      "metadata": {
        "id": "uTcs5CjN4zvB"
      }
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "pqQRPg3d-sCP"
      ],
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}